services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant:/qdrant/storage
    restart: unless-stopped

  # One-shot container that pulls the models once, then exits.
  # This makes your stack reproducible for anyone cloning the repo.
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama_init
    depends_on:
      - ollama
    environment:
      OLLAMA_HOST: http://ollama:11434
    volumes:
      - ollama:/root/.ollama
    entrypoint: ["/bin/sh", "-lc"]
    command: |
      echo "Waiting for Ollama API..."
      until ollama list >/dev/null 2>&1; do
        sleep 2
      done

      echo "Ollama is up. Pulling models..."
      ollama pull deepseek-r1:8b
      ollama pull nomic-embed-text
      echo "Model pulls complete."
    restart: "no"

  app:
    build: ./app
    container_name: agentic_app
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      QDRANT_URL: http://qdrant:6333
      EMBED_MODEL: nomic-embed-text
      CHAT_MODEL: deepseek-r1:8b
    ports:
      - "8000:8000"
    volumes:
      - appdata:/data
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped

  ui:
    build: ./app
    container_name: agentic_ui
    environment:
      API_BASE_URL: http://app:8000
    ports:
      - "8501:8501"
    depends_on:
      - app
    command: ["streamlit", "run", "streamlit_app.py", "--server.address=0.0.0.0", "--server.port=8501"]
    restart: unless-stopped

volumes:
  ollama:
  qdrant:
  appdata: